{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WEEF3H0_TGS"
      },
      "outputs": [],
      "source": [
        "# ---------- 7) Learnable EMHSA layer ----------\n",
        "class EnhancedMultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    EMHSA: Multi-head self-attention with a learnable positive scaling alpha_h per head.\n",
        "    alpha_h = softplus(beta_h) + eps\n",
        "    scores = (Q K^T) / alpha_h\n",
        "    \"\"\"\n",
        "    def __init__(self, units, num_heads=4, eps=1e-6, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        assert units % num_heads == 0, \"units must be divisible by num_heads\"\n",
        "        self.units = units\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = units // num_heads\n",
        "        self.eps = eps\n",
        "\n",
        "        self.W_q = Dense(units)\n",
        "        self.W_k = Dense(units)\n",
        "        self.W_v = Dense(units)\n",
        "        self.W_o = Dense(units)\n",
        "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
        "\n",
        "        # Initialize beta so that alpha â‰ˆ sqrt(depth) (standard scaling)\n",
        "        init_alpha = float(np.sqrt(self.depth))\n",
        "        init_beta = np.log(np.exp(init_alpha) - 1.0)  # approx inverse softplus\n",
        "        self.beta = tf.Variable(\n",
        "            initial_value=tf.ones([self.num_heads], dtype=tf.float32) * init_beta,\n",
        "            trainable=True,\n",
        "            name=\"beta_head\"\n",
        "        )\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])  # (B,H,T,D)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        B = tf.shape(inputs)[0]\n",
        "\n",
        "        Q = self.W_q(inputs)\n",
        "        K_ = self.W_k(inputs)\n",
        "        V = self.W_v(inputs)\n",
        "\n",
        "        Q = self.split_heads(Q, B)\n",
        "        K_ = self.split_heads(K_, B)\n",
        "        V = self.split_heads(V, B)\n",
        "\n",
        "        alpha = tf.nn.softplus(self.beta) + self.eps  # (H,)\n",
        "        alpha = tf.reshape(alpha, (1, self.num_heads, 1, 1))  # broadcast\n",
        "\n",
        "        scores = tf.matmul(Q, K_, transpose_b=True) / alpha\n",
        "        weights = self.softmax(scores)\n",
        "        out = tf.matmul(weights, V)  # (B,H,T,D)\n",
        "\n",
        "        out = tf.transpose(out, perm=[0, 2, 1, 3])  # (B,T,H,D)\n",
        "        out = tf.reshape(out, (B, -1, self.units))  # (B,T,units)\n",
        "\n",
        "        return self.W_o(out)\n"
      ]
    }
  ]
}